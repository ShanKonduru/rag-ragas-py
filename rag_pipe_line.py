import os
from dotenv import load_dotenv
from typing import List

from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser

from datasets import Dataset

from ragas import evaluate
from ragas.metrics import (
    Faithfulness,
    ContextRelevance,
    ContextRecall,
    ContextPrecision
)
import pandas as pd


class RAGPipeline:
    """
    A class for building and evaluating a Retrieval-Augmented Generation (RAG) pipeline.
    """
    def __init__(self, openai_api_key: str, chunk_size: int = 500, chunk_overlap: int = 50, llm_temperature: float = 0.6):
        """
        Initializes the RAGPipeline.

        Args:
            openai_api_key: Your OpenAI API key.
            chunk_size: The size of text chunks for splitting.
            chunk_overlap: The overlap between text chunks.
            llm_temperature: The temperature setting for the language model.
        """
        self.openai_api_key = openai_api_key
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.llm_temperature = llm_temperature
        self.embeddings = OpenAIEmbeddings(openai_api_key=self.openai_api_key)
        self.llm = ChatOpenAI(temperature=self.llm_temperature, openai_api_key=self.openai_api_key)
        self.vector_db = None
        self.retriever = None
        self.chain = None

    def load_knowledge_base(self, file_path: str):
        """
        Loads the knowledge base from a text file.

        Args:
            file_path: The path to the text file.
        """
        loader = TextLoader(file_path, encoding="utf-8")
        document = loader.load()
        return document

    def chunk_data(self, document):
        """
        Chunks the loaded document into smaller pieces.

        Args:
            document: The loaded Langchain Document.

        Returns:
            A list of Langchain Documents representing the chunks.
        """
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size, chunk_overlap=self.chunk_overlap)
        chunks = text_splitter.split_documents(document)
        return chunks

    def create_vector_database(self, chunks: List):
        """
        Creates a vector database from the text chunks.

        Args:
            chunks: A list of Langchain Documents.
        """
        self.vector_db = FAISS.from_documents(chunks, self.embeddings)
        self.retriever = self.vector_db.as_retriever()

    def setup_rag_pipeline(self, prompt_template: str):
        """
        Sets up the Retrieval-Augmented Generation (RAG) pipeline.

        Args:
            prompt_template: The prompt template to use for the RAG pipeline.
        """
        prompt = ChatPromptTemplate.from_template(template=prompt_template)
        self.chain = (
            {"context": self.retriever, "question": RunnablePassthrough()}
            | prompt
            | self.llm
            | StrOutputParser()
        )

    def query(self, question: str) -> str:
        """
        Queries the RAG pipeline with a question.

        Args:
            question: The question to ask.

        Returns:
            The answer generated by the RAG pipeline.
        """
        if self.chain is None:
            raise ValueError("RAG pipeline not set up. Call setup_rag_pipeline() first.")
        return self.chain.invoke(question)

    def generate_questions_and_ground_truths(self, document, num_questions: int = 5):
        """
        Generates a list of questions and corresponding ground truth answers based on the document.
        This is a simplified example and might require a more sophisticated approach
        depending on the complexity of the knowledge base.

        Args:
            document: The loaded Langchain Document.
            num_questions: The number of questions and ground truths to generate.

        Returns:
            A tuple containing two lists: questions and ground_truths.
        """
        # This is a placeholder and would ideally involve a more advanced method
        # potentially using an LLM to generate relevant questions and answers
        # directly from the document content.
        # For now, we'll just take the first few sentences as potential questions
        # and their immediate next sentences as ground truths (very basic).
        questions = []
        ground_truths = []
        all_content = document[0].page_content.split('.')
        for i in range(min(num_questions, len(all_content) - 1)):
            questions.append(all_content[i].strip() + "?")
            ground_truths.append(all_content[i+1].strip() + ".")
        return questions, ground_truths

    def evaluate_rag(self, questions: List[str], ground_truths: List[str]) -> pd.DataFrame:
        """
        Evaluates the RAG pipeline using Ragas metrics.

        Args:
            questions: A list of questions.
            ground_truths: A list of corresponding ground truth answers.

        Returns:
            A Pandas DataFrame containing the evaluation results.
        """
        if self.retriever is None:
            raise ValueError("Vector database and retriever not initialized.")

        answers = []
        contexts = []
        for query in questions:
            answers.append(self.query(query))
            contexts.append([doc.page_content for doc in self.retriever.get_relevant_documents(query)])

        data = {
            "user_input": questions,
            "ground_truth": ground_truths,
            "answer": answers,
            "retrieved_contexts": contexts
        }
        dataset = Dataset.from_dict(data)

        result = evaluate(
            dataset=dataset,
            metrics=[
                Faithfulness(),
                ContextRelevance(),
                ContextRecall(),
                ContextPrecision()
            ],
            llm=self.llm,
            embeddings=self.embeddings
        )
        return result.to_pandas()


def main():
    load_dotenv()
    openai_api_key = os.getenv("OPENAI_API_KEY")

    if not openai_api_key:
        print("OPENAI_API_KEY not found in environment variables.")
        return

    rag_pipeline = RAGPipeline(openai_api_key=openai_api_key)

    # 1. Load the input file to KnowledgeBase
    file_path = "inputs\\state_of_the_union.txt"
    document = rag_pipeline.load_knowledge_base(file_path)
    print("Loaded Document:", document)

    # 2. Chunking the Data
    chunks = rag_pipeline.chunk_data(document)
    print("\nChunked Data (first chunk):", chunks[0])

    # 3. Create Vector Database and Retriever
    rag_pipeline.create_vector_database(chunks)

    # 4. Define Prompt Template
    prompt_template = """You are an assistant for question-answering tasks.
    Use the following piece of retrieved context to answer the question.
    if you don't known the answer, just say that you don't know.
    Use two sentences maximum and keep the answer concise.
    Question: {question}
    Context: {context}
    Answer:
    """

    # 5. Set up RAG Pipeline
    rag_pipeline.setup_rag_pipeline(prompt_template)

    # 6. Test One Question
    test_question = "what did president say about Justic Breyer?"
    result = rag_pipeline.query(test_question)
    print(f"\nQuestion: {test_question}")
    print(f"Answer: {result}")

    # 7. Generate Questions and Ground Truths (basic example)
    questions, ground_truths = rag_pipeline.generate_questions_and_ground_truths(document, num_questions=6)
    print("\nGenerated Questions:", questions)
    print("Generated Ground Truths:", ground_truths)

    # 8. Evaluate RAG Pipeline
    if rag_pipeline.retriever:
        evaluation_df = rag_pipeline.evaluate_rag(questions, ground_truths)
        print("\nRAG Evaluation Results:")
        print(evaluation_df)
    else:
        print("\nRetriever not initialized, skipping evaluation.")


if __name__ == "__main__":
    main()